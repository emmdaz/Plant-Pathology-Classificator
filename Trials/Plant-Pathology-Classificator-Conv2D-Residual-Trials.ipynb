{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Activation, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the GPU \n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "    \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is a no-op as it is now the default behavior.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmdaz\u001b[0m (\u001b[33memmdaz-zzz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  healthy  multiple_diseases  rust  scab\n",
       "0  Train_0        0                  0     0     1\n",
       "1  Train_1        0                  1     0     0\n",
       "2  Train_2        1                  0     0     0\n",
       "3  Train_3        0                  0     1     0\n",
       "4  Train_4        1                  0     0     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de los datos \n",
    "\n",
    "df = pd.read_csv(\"/Plant-Pathology-Classificator/plant-pathology-2020-/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df[\"label\"] = df[[\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]].idxmax(axis=1)\n",
    "\n",
    "df[\"filepath\"] = df['image_id'].apply(lambda x: os.path.join(\"/Plant-Pathology-Classificator/plant-pathology-2020-/images\", f'{x}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 910\n",
      "Validation size: 365\n",
      "Test size: 546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size = 0.5, stratify = df[\"label\"], random_state = 4)\n",
    "\n",
    "X_test, X_val = train_test_split(X_temp, test_size = 0.4, stratify = X_temp[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small training size: 364\n",
      "Small val size: 292\n"
     ]
    }
   ],
   "source": [
    "mini_train, _ = train_test_split(X_train, test_size = 0.6, stratify = X_train[\"label\"], random_state = 4)\n",
    "\n",
    "mini_val, _ = train_test_split(X_val, test_size = 0.2, stratify = X_val[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Small training size:\", len(mini_train))\n",
    "print(\"Small val size:\", len(mini_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(\n",
    "    rotation_range = 0.1,\n",
    "    brightness_range = (0.25,1.5),\n",
    "    channel_shift_range = 20.0,\n",
    "    fill_mode = \"nearest\",\n",
    "    cval = 128,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    "    rescale=1./255,\n",
    "    dtype = \"float32\"\n",
    "    )\n",
    "\n",
    "datagen_test_and_val = ImageDataGenerator(rescale=1./255, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 910 validated image filenames belonging to 4 classes.\n",
      "Found 546 validated image filenames belonging to 4 classes.\n",
      "Found 365 validated image filenames belonging to 4 classes.\n",
      "Found 364 validated image filenames belonging to 4 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mini_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 34\u001b[0m\n\u001b[1;32m     17\u001b[0m val \u001b[38;5;241m=\u001b[39m datagen_test_and_val\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[1;32m     18\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m X_val,\n\u001b[1;32m     19\u001b[0m     x_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m mini_train \u001b[38;5;241m=\u001b[39m datagen_train\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[1;32m     26\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m mini_train,\n\u001b[1;32m     27\u001b[0m     x_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m mini_val \u001b[38;5;241m=\u001b[39m datagen_test_and_val\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m---> 34\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mmini_test\u001b[49m,\n\u001b[1;32m     35\u001b[0m     x_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m     y_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m     target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m     38\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mini_test' is not defined"
     ]
    }
   ],
   "source": [
    "train = datagen_train.flow_from_dataframe(\n",
    "    dataframe = X_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test = datagen_test_and_val.flow_from_dataframe(\n",
    "    dataframe = X_test,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val = datagen_test_and_val.flow_from_dataframe(\n",
    "    dataframe = X_val,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_train = datagen_train.flow_from_dataframe(\n",
    "    dataframe = mini_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_val = datagen_test_and_val.flow_from_dataframe(\n",
    "    dataframe = mini_val,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    target_size = (128,128),\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para los bloques residuales\n",
    "\n",
    "def residual_block(x, kernel, kernel_size, activation, dropout, dropout_rate, regularizer, r_2):\n",
    "        \n",
    "    residual = x  \n",
    "        \n",
    "    if dropout == \"y\":\n",
    "        # Camino \"principal\"\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                              activation = activation)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Capa intermedia Dropot\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "            \n",
    "        # Capa lineal\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "    else: \n",
    "        # Camino \"principal\"\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                                  activation = activation, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "                \n",
    "        # Capa lineal\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                                  kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "#     if x.shape[-1] != residual.shape[-1]:\n",
    "#         residual = layers.Conv2D(filters = kernel, kernel_size = (1, 1),\n",
    "#                                  strides=strides, padding = \"same\")(residual)\n",
    "#         residual = layers.BatchNormalization()(residual)\n",
    "\n",
    "    # Suma de la conexión residual\n",
    "    x = layers.add([x, residual]) \n",
    "    x = layers.Activation(activation)(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = models.Sequential()\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    # x = data_augmentation(inputs, training = True)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Optuna sugiere función de activación para todas las capas\n",
    "    activation = \"relu\"\n",
    "    \n",
    "    # Optuna sugiere regularizador\n",
    "    regularizer = \"L2\"\n",
    "    r_2 = trial.suggest_float(\"regularizer_value_2\", 1e-6, 3e-6, log = True)\n",
    "    \n",
    "    # Optuna sugiere el número de capas\n",
    "    n_layers = trial.suggest_int(\"N_layers\", 10, 15)\n",
    "    \n",
    "    # Optuna sugiere número de kernels y su tamaño en la primer capa convolucional\n",
    "    \n",
    "    kernel_1 = 10\n",
    "    size_1 = trial.suggest_categorical(\"Kernel_Size_1\", [2,7])\n",
    "    \n",
    "    # Optuna sugiere Learning Rate y Optimizador\n",
    "    \n",
    "    lr = trial.suggest_float(\"learning_rate\", 2.5e-4, 1e-3, log = True)\n",
    "    \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\"])\n",
    "    \n",
    "                              \n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "                              \n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Primera convolución\n",
    "    x = layers.Conv2D(kernel_1, (size_1,size_1), padding = \"same\")(inputs)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    kernel_per_layer = [kernel_1]\n",
    "    kernel_size_per_layer = [size_1]\n",
    "    \n",
    "    # Optuna sugiere número de kernels su tamaño y función de activación; también sugiere Dropout\n",
    "    # y regularizadores\n",
    "    \n",
    "    dropout_per_layer = []\n",
    "    dropout_percentage_per_layer = []\n",
    "    \n",
    "    n_kernel =  trial.suggest_int(\"Kernels\", 12,16)\n",
    "    \n",
    "    kernel_size = trial.suggest_categorical(f\"Kernel_Size\", [3,5])\n",
    "    kernel_size_per_layer.append(kernel_size)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "                              \n",
    "        dropout = trial.suggest_categorical(f\"Dropout_L{i+2}\", [\"y\", \"n\"])\n",
    "        dropout_per_layer.append(dropout)\n",
    "                              \n",
    "        dropout_rate = trial.suggest_float(f\"Dropout_value_L{i+2}\",0.1, 0.2)\n",
    "        \n",
    "        # Capa Convolucional i-ésima:\n",
    "        \n",
    "        # Se elige entre Dropout o un Regularizador\n",
    "        if dropout == \"y\":\n",
    "            \n",
    "            dropout_percentage_per_layer.append(dropout_rate)\n",
    "            \n",
    "            ker = int(n_kernel * (2 ** (i // 2)))\n",
    "            kernel_per_layer.append(ker)\n",
    "            \n",
    "            x = layers.Conv2D(ker, (kernel_size, kernel_size), strides = 2, padding = \"same\",\n",
    "                              activation = activation)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "            x = residual_block(x, ker, kernel_size, activation, dropout, dropout_rate, regularizer, r_2)\n",
    "            \n",
    "        else:\n",
    "            dropout_percentage_per_layer.append(0.0)\n",
    "\n",
    "            ker = int(n_kernel * (2 ** (i // 2)))\n",
    "            kernel_per_layer.append(ker)\n",
    "\n",
    "            x = layers.Conv2D(ker, (kernel_size, kernel_size), strides = 2, padding = \"same\",\n",
    "                                  activation = activation, kernel_regularizer = regularizers.l2(r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "            x = residual_block(x, ker, kernel_size, activation, dropout, dropout_rate, regularizer, r_2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "                              \n",
    "    outputs = layers.Dense(4, activation = \"softmax\", dtype = \"float32\")(x)\n",
    "        \n",
    "    model = models.Model(inputs, outputs)\n",
    "                              \n",
    "    model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = \"categorical_crossentropy\",\n",
    "        metrics = [\"accuracy\"])\n",
    "    \n",
    "    #############################################################################################################\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"Plant-Pathology-Classificator-Conv2D-Residual-Trials-8.0\",\n",
    "        name = f\"Trial_{trial.number}\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"kernel_1\": kernel_1,\n",
    "            \"size_1\": size_1,\n",
    "            \"activation\": activation,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"kernel_per_layer\": kernel_per_layer,\n",
    "            \"kernel_size_per_layer\": kernel_size_per_layer,\n",
    "            \"regularizer\": regularizer,\n",
    "            \"r_value2\": r_2,\n",
    "            \"dropout_per_layer\": dropout_per_layer,\n",
    "            \"dropout_percentage_per_layer\": dropout_percentage_per_layer,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": optimizer_name,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #############################################################################################################\n",
    "                              \n",
    "    early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)\n",
    "    # lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience = 5)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \"\"\"\n",
    "    Creación del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(model.summary())\n",
    "    \n",
    "        history = model.fit(\n",
    "            mini_train,\n",
    "            validation_data = mini_val,\n",
    "            epochs = 200,\n",
    "            batch_size = 32,\n",
    "            verbose = 1, \n",
    "            callbacks = [WandbMetricsLogger(log_freq = 5), early_stopping]\n",
    "        )\n",
    "\n",
    "        # val_loss = min(history.history[\"val_loss\"])\n",
    "        # train_loss = min(history.history[\"loss\"])\n",
    "        val_accuracy = max(history.history[\"val_accuracy\"])\n",
    "    \n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(f\"Intento {trial.number} falló debido a: {e}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        wandb.finish()\n",
    "        gc.collect()\n",
    "        \n",
    "        return float(\"inf\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Intento {trial.number} falló. Unexpected error: {e}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        wandb.finish()\n",
    "        gc.collect()\n",
    "        \n",
    "        return float(\"inf\")\n",
    "\n",
    "    # Penalize overfitting\n",
    "    \n",
    "    # score = val_loss + 0.1 * (train_loss - val_loss)\n",
    "    \n",
    "    score = val_accuracy\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    wandb.finish()\n",
    "\n",
    "    return 1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name = \"Experimentos-Serie-8.0\", direction = \"minimize\")\n",
    "study.optimize(objective, n_trials = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Número de pruebas terminadas: \", len(study.trials))\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Mejor intento: \", trial)\n",
    "\n",
    "print(\"Valor: \", trial.value)\n",
    "print(\"Hiperparámetros: \", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
