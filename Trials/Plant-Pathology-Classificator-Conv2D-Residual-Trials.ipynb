{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Activation, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the GPU \n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "    \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is a no-op as it is now the default behavior.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmdaz\u001b[0m (\u001b[33memmdaz-zzz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  healthy  multiple_diseases  rust  scab\n",
       "0  Train_0        0                  0     0     1\n",
       "1  Train_1        0                  1     0     0\n",
       "2  Train_2        1                  0     0     0\n",
       "3  Train_3        0                  0     1     0\n",
       "4  Train_4        1                  0     0     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de los datos \n",
    "\n",
    "df = pd.read_csv(\"/Plant-Pathology-Classificator/plant-pathology-2020-/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df[\"label\"] = df[[\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]].idxmax(axis=1)\n",
    "\n",
    "df[\"filepath\"] = df['image_id'].apply(lambda x: os.path.join(\"/Plant-Pathology-Classificator/plant-pathology-2020-/images\", f'{x}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1274\n",
      "Test size: 364\n",
      "Validation size: 183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size = 0.3, stratify = df[\"label\"], random_state = 4)\n",
    "\n",
    "X_test, X_val = train_test_split(X_temp, test_size = 1/3, stratify = X_temp[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n",
    "print(\"Validation size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small training size: 382\n",
      "Small test size: 182\n"
     ]
    }
   ],
   "source": [
    "mini_train, _ = train_test_split(X_train, test_size = 0.7, stratify = X_train[\"label\"], random_state = 4)\n",
    "\n",
    "mini_test, _ = train_test_split(X_test, test_size = 0.5, stratify = X_test[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Small training size:\", len(mini_train))\n",
    "print(\"Small test size:\", len(mini_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1274 validated image filenames belonging to 4 classes.\n",
      "Found 364 validated image filenames belonging to 4 classes.\n",
      "Found 183 validated image filenames belonging to 4 classes.\n",
      "Found 382 validated image filenames belonging to 4 classes.\n",
      "Found 182 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train = datagen.flow_from_dataframe(\n",
    "    dataframe = X_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (128, 128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test = datagen.flow_from_dataframe(\n",
    "    dataframe = X_test,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    image_size = (128, 128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val = datagen.flow_from_dataframe(\n",
    "    dataframe = X_val,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (128, 128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_train = train = datagen.flow_from_dataframe(\n",
    "    dataframe = mini_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (128, 128),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_test = train = datagen.flow_from_dataframe(\n",
    "    dataframe = mini_test,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (128, 128),\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'RandomConstrast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Capa de Data augmentation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data_augmentation \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      4\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomBrightness(\u001b[38;5;241m0.1\u001b[39m, value_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m), seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomConstrast\u001b[49m(\u001b[38;5;241m0.1\u001b[39m, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      6\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomFlip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal_and_vertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      7\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomRotation((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.2\u001b[39m), seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, value_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[1;32m      8\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'RandomConstrast'"
     ]
    }
   ],
   "source": [
    "# Capa de Data augmentation\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomBrightness(0.1, value_range = (0.0, 1.0), seed = 4),\n",
    "    layers.RandomContrast(0.1, seed = 4),\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", seed = 4),\n",
    "    layers.RandomRotation((-0.1,0.2), seed = 4, value_range=(0.0, 1.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para los bloques residuales\n",
    "\n",
    "def residual_block(x, kernel, kernel_size, activation, dropout, dropout_rate, regularizer, r_1, r_2):\n",
    "        \n",
    "    residual = x  \n",
    "        \n",
    "    if dropout == \"y\":\n",
    "        # Camino \"principal\"\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                              activation = activation)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        # Capa lineal\n",
    "        x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "            \n",
    "    else: \n",
    "        if regularizer == \"L2\":\n",
    "            # Camino \"principal\"\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                                  activation = activation, kernel_regularizer = regularizers.L2(r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "                \n",
    "            # Capa lineal\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                                  kernel_regularizer = regularizers.L2(r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "        else:\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                                  activation = activation, kernel_regularizer = regularizers.L1L2(r_1,r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "                \n",
    "            # Capa lineal\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), padding = \"same\",\n",
    "                              kernel_regularizer = regularizers.L1L2(r_1, r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Suma de la conexión residual\n",
    "    x = layers.add([x, residual]) \n",
    "    x = layers.Activation(activation)(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = models.Sequential()\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    x = data_augmentation(inputs, training = True)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Optuna sugiere función de activación para todas las capas\n",
    "    activation = trial.suggest_categorical(\"Activation\", [\"relu\", \"relu6\", \"leaky_relu\"])\n",
    "    \n",
    "    # Optuna sugiere regularizador\n",
    "    regularizer = trial.suggest_categorical(\"Regularizer\", [\"L2\",\"L1L2\"])\n",
    "    r_1 = trial.suggest_float(\"regularizer_value\", 2.8e-6, 4e-6, log = True)\n",
    "    r_2 = trial.suggest_float(\"regularizer_value_2\", 1e-7, 4.5e-7, log = True)\n",
    "    \n",
    "    # Optuna sugiere el número de capas\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 20, 25)\n",
    "    \n",
    "    # Optuna sugiere número de kernels y su tamaño en la primer capa convolucional\n",
    "    \n",
    "    kernel_1 = trial.suggest_int(\"Kernel_1\", 8, 10)\n",
    "    size_1 = trial.suggest_categorical(\"Kernel_Size_1\", [7,8])\n",
    "    \n",
    "    # Optuna sugiere Learning Rate y Optimizador\n",
    "    \n",
    "    lr = trial.suggest_float(\"learning_rate\", 3e-4, 1e-3, log = True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    \n",
    "    # Primera convolución\n",
    "    x = layers.Conv2D(kernel_1, (size_1,size_1), padding = \"same\")(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    \n",
    "    # Primer bloque residual\n",
    "                              \n",
    "    x = residual_block(x, kernel_1, size_1, activation, \"n\", 0.0, regularizer, r_1, r_2)\n",
    "    \n",
    "    kernel_per_layer = [kernel_1]\n",
    "    kernel_size_per_layer = [size_1]\n",
    "    \n",
    "    # Optuna sugiere número de kernels su tamaño y función de activación; también sugiere Dropout\n",
    "    # y regularizadores\n",
    "    \n",
    "    dropout_per_layer = []\n",
    "    dropout_percentage_per_layer = []\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        kernel = trial.suggest_int(f\"Kernel_{i+2}\", 10, 20)\n",
    "        kernel_per_layer.append(kernel)\n",
    "        \n",
    "        kernel_size = trial.suggest_categorical(f\"Kernel_Size_{i+2}\", [3,5])\n",
    "        kernel_size_per_layer.append(kernel_size)    \n",
    "                              \n",
    "        dropout = trial.suggest_categorical(f\"Dropout_L{i+2}\", [\"y\", \"n\"])\n",
    "        dropout_per_layer.append(dropout)\n",
    "                              \n",
    "        dropout_rate = trial.suggest_float(f\"Dropout_value_L{i+2}\",0.1, 0.3)\n",
    "        \n",
    "        # Se elige entre Dropout o un Regularizador\n",
    "        if dropout == \"y\":\n",
    "            dropout_percentage_per_layer.append(dropout_rate)\n",
    "        else:\n",
    "            dropout_percentage_per_layer.append(0.0)\n",
    "        \n",
    "        # Capa Convolucional i-ésima\n",
    "        if dropout == \"n\":\n",
    "            if regularizer == \"L2\":\n",
    "                x = layers.Conv2D(kernel, (kernel_size, kernel_size), strides = 2, padding = \"same\",\n",
    "                              activation = activation, kernel_regularizer = regularizers.L2(r_2))(x)\n",
    "            else: \n",
    "                x = layers.Conv2D(kernel, (kernel_size, kernel_size), strides = 2, padding = \"same\",\n",
    "                                  activation = activation,kernel_regularizer = regularizers.L1L2(r_1,r_2))(x)\n",
    "    \n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Bloque residual i-ésimo\n",
    "        x = residual_block(x, kernel, kernel_size, activation, dropout, dropout_rate, regularizer, r_1, r_2)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "                              \n",
    "    outputs = layers.Dense(4, activation = \"softmax\")(x)\n",
    "        \n",
    "    model = models.Model(inputs, outputs)\n",
    "                              \n",
    "    model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = \"categorical_crossentropy\",\n",
    "        metrics = [\"accuracy\"])\n",
    "    \n",
    "    #############################################################################################################\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"Plant-Pathology-Classificator-Conv2D-Residual-Trials-4.0\",\n",
    "        name = f\"Trial_{trial.number}\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"kernel_1\": kernel_1,\n",
    "            \"size_1\": size_1,\n",
    "            \"activation_1\": activation_1,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"kernel_per_layer\": kernel_per_layer,\n",
    "            \"kernel_size_per_layer\": kernel_size_per_layer,\n",
    "            \"activations_per_layer\": activation_per_layer,\n",
    "            \"regularizer\": regularizer,\n",
    "            \"r_value\": r_1,\n",
    "            \"r_value2\": r_2,\n",
    "            \"dropout_per_layer\": dropout_per_layer,\n",
    "            \"dropout_percentage_per_layer\": dropout_percentage_per_layer,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": \"RMSprop\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #############################################################################################################\n",
    "                              \n",
    "    early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience = 5)\n",
    "    \n",
    "    try:\n",
    "        history = model.fit(\n",
    "            mini_train,\n",
    "            validation_data = mini_test,\n",
    "            epochs = 200,\n",
    "            batch_size = 32,\n",
    "            verbose = 1, \n",
    "            callbacks = [WandbMetricsLogger(log_freq = 5), early_stopping, lr_reduction]\n",
    "        )\n",
    "\n",
    "        val_loss = min(history.history[\"val_loss\"])\n",
    "        train_loss = min(history.history[\"loss\"])\n",
    "        val_accuracy = max(history.history[\"val_accuracy\"])\n",
    "    \n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        tf.keras.backend.clear_session()\n",
    "        wandb.finish()\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # Penalize overfitting\n",
    "    # score = val_loss + 0.1 * (train_loss - val_loss)\n",
    "    score = val_accuracy\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    wandb.finish()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name = \"Experimentos-Serie-1.0\", direction = \"maximize\")\n",
    "study.optimize(objective, n_trials = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Número de pruebas terminadas: \", len(study.trials))\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Mejor intento: \", trial)\n",
    "\n",
    "print(\"Valor: \", trial.value)\n",
    "print(\"Hiperparámetros: \", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
