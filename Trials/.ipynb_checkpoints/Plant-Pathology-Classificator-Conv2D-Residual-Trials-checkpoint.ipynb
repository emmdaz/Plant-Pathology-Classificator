{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Activation, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the GPU \n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU \\n\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is a no-op as it is now the default behavior.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmdaz\u001b[0m (\u001b[33memmdaz-zzz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  healthy  multiple_diseases  rust  scab\n",
       "0  Train_0        0                  0     0     1\n",
       "1  Train_1        0                  1     0     0\n",
       "2  Train_2        1                  0     0     0\n",
       "3  Train_3        0                  0     1     0\n",
       "4  Train_4        1                  0     0     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de los datos \n",
    "\n",
    "df = pd.read_csv(\"/Plant-Pathology-Classificator/plant-pathology-2020-/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# convert one-hot columns to a single class name\n",
    "df[\"label\"] = df[[\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]].idxmax(axis=1)\n",
    "\n",
    "df[\"filepath\"] = df['image_id'].apply(lambda x: os.path.join(\"/Plant-Pathology-Classificator/plant-pathology-2020-/images\", f'{x}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1274\n",
      "Test size: 364\n",
      "Validation size: 183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size = 0.3, stratify = df[\"label\"], random_state = 4)\n",
    "\n",
    "X_test, X_val = train_test_split(X_temp, test_size = 1/3, stratify = X_temp[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n",
    "print(\"Validation size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small training size: 254\n",
      "Small training size: 91\n"
     ]
    }
   ],
   "source": [
    "mini_train, _ = train_test_split(X_train, test_size = 0.8, stratify = X_train[\"label\"], random_state = 4)\n",
    "\n",
    "mini_test, _ = train_test_split(X_test, test_size = 0.75, stratify = X_test[\"label\"], random_state = 4)\n",
    "\n",
    "print(\"Small training size:\", len(mini_train))\n",
    "print(\"Small training size:\", len(mini_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1274 validated image filenames belonging to 4 classes.\n",
      "Found 364 validated image filenames belonging to 4 classes.\n",
      "Found 183 validated image filenames belonging to 4 classes.\n",
      "Found 254 validated image filenames belonging to 4 classes.\n",
      "Found 91 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train = datagen.flow_from_dataframe(\n",
    "    dataframe = X_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test = datagen.flow_from_dataframe(\n",
    "    dataframe = X_test,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val = datagen.flow_from_dataframe(\n",
    "    dataframe = X_val,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_train = train = datagen.flow_from_dataframe(\n",
    "    dataframe = mini_train,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "mini_test = train = datagen.flow_from_dataframe(\n",
    "    dataframe = mini_test,\n",
    "    x_col = 'filepath',\n",
    "    y_col = 'label',\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = models.Sequential()\n",
    "    inputs = layers.Input(shape=(256, 256, 3))\n",
    "    \n",
    "    # Optuna sugiere número de kernels y su tamaño en la primer capa convolucional,\n",
    "    # así como su función de activación\n",
    "    \n",
    "    kernel_1 = trial.suggest_int(\"Kernel_1\", 10, 128)\n",
    "    size_1 = trial.suggest_categorical(\"Kernel_Size_1\", [3,7,8,10])\n",
    "    activation_1 = trial.suggest_categorical(\"Activation_1\", [\"relu\", \"relu6\", \"selu\", \"leaky_relu\", \"relu6\"])\n",
    "    \n",
    "    # Primera convolución\n",
    "    x = layers.Conv2D(kernel_1, (size_1,size_1), padding = \"same\")(inputs)\n",
    "    x = layers.Activation(activation_1)(x)\n",
    "    \n",
    "    # Función para los bloques residuales\n",
    "    def residual_block(x, kernel, kernel_size, activation, dropout, dropout_rate, regularizer, r_1, r_2):\n",
    "        \n",
    "        residual = x  \n",
    "\n",
    "        # Camino \"principal\"\n",
    "        \n",
    "        if regularizer == \"L1\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\",\n",
    "                              activation = activation, kernel_regularizer = regularizers.L1(r_1))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        elif regularizer == \"L2\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\",\n",
    "                              activation = activation, kernel_regularizer = regularizers.L2(r_2)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        elif regularizer == \"L1L2\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\",\n",
    "                              activation = activation, kernel_regularizer = regularizers.L1L2(r_1,r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\", activation = activation)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Capa intermedia Dropout                      \n",
    "        if dropout == \"y\":\n",
    "            \n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Segunda capa Convolicional, lineal\n",
    "        if regularizer == \"L1\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\",\n",
    "                              kernel_regularizer = regularizers.L1(r_1))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        elif regularizer == \"L2\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\", \n",
    "                              kernel_regularizer = regularizers.L2(r_2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            \n",
    "        elif regularizer == \"L1L2\":\n",
    "            x = layers.Conv2D(filters, (kernel, kernel), padding = \"same\",\n",
    "                              kernel_regularizer = regularizers.L1L2(r_1, r2))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Suma de la conexión residual\n",
    "        x = layers.add([x, residual]) # Capa que suma elemento a elemento\n",
    "        x = layers.Activation()(x)\n",
    "        return x\n",
    "    \n",
    "    # Primer bloque residual\n",
    "                              \n",
    "    x = residual_block(x, kernel_1, size_1, activation_1)\n",
    "    \n",
    "    kernel_per_layer = [kernel_1]\n",
    "    kernel_size_per_layer = [size_1]\n",
    "    activation_per_layer = [activation_1]\n",
    "    \n",
    "    # Optuna sugiere número de capas, número de kernels su tamaño y función de activación; también sugiere Dropout\n",
    "    # y regularizadores\n",
    "                              \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 10, 50)\n",
    "    \n",
    "    dropout_per_layer = []\n",
    "    dropout_percentage_per_layer = []\n",
    "    \n",
    "    regularizer = trial.suggest_categorical(\"Regularizer\", [\"L1\",\"L2\",\"L1L2\"])\n",
    "    r_1 = trial.suggest_float(\"regularizer_value\", 1e-6, 1e-4, log = True)\n",
    "    r_2 = trial.suggest_float(\"regularizer_value_2\", 1e-6, 1e-4, log = True)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        kernel = trial.suggest_int(f\"Kernel_{i+2}\", 10, 128)\n",
    "        kernel_per_layer.append(kernel)\n",
    "        \n",
    "        kernel_size = trial.suggest_categorical(f\"Kernel_Size_{i+2}\", [3,7,8,10])\n",
    "        kernel_size_per_layer.append(kernel_size)\n",
    "            \n",
    "        activation = trial.suggest_categorical(f\"Activation_{i+2}\", [\"relu\", \"relu6\", \"selu\", \"leaky_relu\", \"relu6\"])      \n",
    "        activation_per_layer.append(activation)\n",
    "                              \n",
    "        dropout = trial.suggest_categorical(f\"Dropout_L{i+2}\", [\"y\", \"n\"])\n",
    "        dropout_per_layer.append(dropout)\n",
    "                              \n",
    "        dropout_rate = trial.suggest_float(f\"Dropout_value_L{i+2}\",0.2, 0.5)\n",
    "        \n",
    "        if dropout == \"y\":\n",
    "            dropout_percentage_per_layer.append(dropout_rate)\n",
    "        else:\n",
    "            dropout_percentage_per_layer.append(0.0)\n",
    "        \n",
    "        # Capa Convolucional i-ésima\n",
    "        if regularizer == \"L1\":\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), strides = 2, padding = \"same\", activation = activation,\n",
    "                          kernel_regularizer = regularizers.L1(r_1))(x)\n",
    "        elif regularizer == \"L2\":\n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), strides = 2, padding = \"same\", activation = activation,\n",
    "                          kernel_regularizer = regularizers.L2(r_2))(x)\n",
    "        else: \n",
    "            x = layers.Conv2D(kernel, (kernel_size, kernel_size), strides = 2, padding = \"same\", activation = activation,\n",
    "                          kernel_regularizer = regularizers.L1L2(r_1,r_2))(x)\n",
    "    \n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Bloque residual i-ésimo\n",
    "        x = residual_block(x, kernel, kernel_size, activation)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "                              \n",
    "    outputs = layers.Dense(4, activation = \"softmax\")(x)\n",
    "\n",
    "    # Optuna sugiere Learning Rate y Optimizador\n",
    "    \n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\", \"rmsprop\"])\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "                              \n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "                              \n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr)\n",
    "        \n",
    "        \n",
    "    model = models.Model(inputs, outputs)\n",
    "                              \n",
    "    model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = \"categorical_crossentropy\",\n",
    "        metrics = [\"accuracy\"])\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"Global-House-Purchase-Decision-ANN-Trials-4\",\n",
    "        name = f\"Trial_{trial.number}\",\n",
    "        reinit = True,\n",
    "        config = {\n",
    "            \"n_layers\": n_layers,\n",
    "            \"units_per_layer\": units_per_layer,\n",
    "            \"activations_per_layer\": activations_per_layer,\n",
    "            \"regularizer\": regularizer,\n",
    "            \"r_value\": r_value,\n",
    "            \"r_value2\": r_value2,\n",
    "            \"dropout_per_layer\": dropout_per_layer,\n",
    "            \"dropout_percentage_per_layer\": dropout_percentage_per_layer,\n",
    "            \"learning_rate\": lr,\n",
    "            \"optimizer\": optimizer_name\n",
    "        }\n",
    "    )\n",
    "                              \n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 8, restore_best_weights = True)\n",
    "                              \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        verbose = 0, \n",
    "        callbacks = [WandbMetricsLogger(log_freq=5), early_stopping]\n",
    "    )\n",
    "\n",
    "    val_loss = min(history.history[\"val_loss\"])\n",
    "    train_loss = min(history.history[\"loss\"])\n",
    "\n",
    "    # Penalize overfitting\n",
    "    score = val_loss + 0.1 * (train_loss - val_loss)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name = \"Proyecto\", direction = \"minimize\")\n",
    "study.optimize(objective, n_trials = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de pruebas terminadas: \", len(study.trials))\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Mejor intento: \", trial)\n",
    "\n",
    "print(\"Valor: \", trial.value)\n",
    "print(\"Hiperparámetros: \", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
